%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Assignment
% LaTeX Template
% Version 2.0 (12/1/2019)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{scrartcl} % Font size

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{20pt} % Whitespace
	{\huge Gofinge Note for EM algorithm}\\ % The assignment title
	\vspace{12pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{12pt} % Whitespace
}

\author{\LARGE Xiaoyang Wu} % Your name

\date{\normalsize February 21, 2019} % Today's date (\today) or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	 INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction to expectation-maximization(EM)  algorithm}

\subsection{An example of EM algorithm}

Let's start with an example from Chuong B Do \& Serafim Batzoglouâ€˜s tutorial paper \textit{What is the expectation maximization algorithm?}, and I have saved the .pdf file of this paper in the \textit{material} folder.

\medskip

\begin{figure}[h] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.5\columnwidth]{example.png} % Example image
	\caption{Example: As an example, consider a simple coin-flipping experiment in which we are given a pair of coins A and B of unknown biases, $\theta_A$ and $\theta_B$, respectively. Our goal is to estimate $\theta = (\theta_A, \theta_B)$ by repeating the following procedure five times: randomly choose one of the two coins (with equal probability), and perform ten independent coin tosses with the selected coin. Thus, the entire procedure involves a total of 50 coin tosses'}
\end{figure}

If we can figure out whether the coin we flipped is A or B, we can estimate $\theta_A$ and $\theta_B$ through one of our most familiar friends, \textbf{MLE}.(Figure 1.1a) But actually, since we can not figure it out, MLE will not work in this situation. Then EM algorithm come on the stage. EM algorithm is simple because it just contain two steps in the Iteration and EM algorithm is complex is complex for its mathematical proof and further stretch in theory and application.(Figure 1.1b)

\medskip

The first step of the algorithm is \textbf{Expectation step}(E step). In this step, we start by assuming $\hat{\theta}_A^{(0)} = 0.60$ and $\hat{\theta}_B^{(0)} = 0.50$, wether the assumption is true is not important. What is significant is that we can compute the expectation of Coin A and Coin B in each ten tosses with the selected coin. and compute the expectation...

\medskip

\subsection{Two steps in iteration of EM}
\textbf{Notation:} Given the statistical model which generates a set X of observed data(50 samples in coin-flipping experiment), a set of unobserved latent data or missing values Z(the coin is A or B), and a vector of unknown parameters $\theta$, along with a likelihood function $L(\theta; X, Z) = p(X, Z| \theta)$,  and the marginal likelihood function of the observed data is given by:

$$L(\theta; X) = p(X| \theta) = \int p(X, Z| \theta)dZ$$

However, this quantity is often intractable (we don't know the coin we tossed is A or B). 

\medskip

Then the EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying the following two steps.

\textbf{(a) Expectation Step:} 

%----------------------------------------------------------------------------------------

\end{document}
